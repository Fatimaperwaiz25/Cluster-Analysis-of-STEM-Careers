---
title: "Cluster Analysis of STEM Career Interest"
author: "Fatima"
date: "2024-04-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

### **INTRODUCTION**
This project focuses on identifying the underlying clusters in students career interests. Participants were ask to rate different STEM careers on a Likert Scale and a hierarchical cluster analysis was run to find optimal number of clusters. Hierarchical clustering is an alternate method to k-mean cluster and it does not require specifying the expected number of clusters.

### **R Packages Needed**

```{r}
#Cluster Analysis
library(dplyr) 
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(data.table)
```

### **Loading and Prepapring Data**
```{r}
#load data, , row.names=1
career_data1<-read.csv("/Users/fatimaperwaizkhan/Documents/NSF Research Work/Results/careerdata_tp.csv", header = FALSE, sep=",")
career_data <- na.omit(career_data1)
#transpose data frame
career_data <- transpose(career_data1)
#career_data <- scale(career_data)
head(career_data)
```
### **Agglomerative Hierarchical Clustering**
To perform agglomerative hierarchical clustering, first we will calculate the dissimilarity matrix using one of the agglomeration method (i.e. “complete”, “average”, “single”, “ward.D”). After which, we will plot the dendrograms. If we are interested in finding the Agglomerative coefficient, we can use the function agnes. Agglomerative coefficient tells the amount of clustering found in our structure. 
Here we are implementing all the agglomeration methods to see which one gives the best results.
```{r}
# Dissimilarity matrix
d <- dist(career_data, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1, main = 'Cluster Analysis of Careers (Hierarchical)')

# Compute with agnes
hc2 <- agnes(career_data, method = "complete")
pltree(hc2, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 
# Agglomerative coefficient
hc2$ac


# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(career_data, method = x)$ac
}

map_dbl(m, ac)

hc3 <- agnes(career_data, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 
hc3$ac

# Cut tree into 2 groups
sub_grp <- cutree(hc3, k = 2)
# Number of members in each cluster
table(sub_grp)
career_data_mod <- career_data %>% mutate( cluster = sub_grp) 
fviz_cluster(list(data = career_data, cluster = sub_grp))
```

### **Optimal Number of Clusters**
The next important thing to find is what is the optimal number of clusters that your data shows. We utilize silhoutte and elbow method to find that 2 clusters best define our data.
```{r}
#finding optimal no. of clusters
#silhoutte method
fviz_nbclust(career_data, kmeans, method = "silhouette")
#elbow method
fviz_nbclust(career_data, kmeans, method = "wss")

# compute gap statistic
set.seed(123)
gap_stat <- clusGap(career_data, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
```
```{r }
#extra 
#Compute k-means clustering with k = 2
set.seed(123)
final <- kmeans(career_data, 2, nstart = 25)
print(final)

career_data %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 2, border = 2:5)

# Cut agnes() tree into 2 groups
hc_a <- agnes(career_data, method = "ward")
cutree(as.hclust(hc_a), k = 2)

# Cut diana() tree into 4 groups
hc_d <- diana(career_data)
cutree(as.hclust(hc_d), k = 4)

```

